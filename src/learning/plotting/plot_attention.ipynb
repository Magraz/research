{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "from learning.plotting.utils import (\n",
    "    plot_attention_heatmap,\n",
    "    plot_attention_time_series,\n",
    "    plot_attention_over_time_grid,\n",
    "    plot_key_attention_trends,\n",
    "    plot_token_attention_trends,\n",
    "    plot_node_attention_trends,\n",
    "    plot_all_attention_heads,\n",
    "    plot_transformer_attention\n",
    ")\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "plotting_dir = Path().resolve()\n",
    "config_dir = plotting_dir / \"ppo_config.yaml\"\n",
    "\n",
    "with open(config_dir, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "data = []\n",
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Use a different color for each experiment-batch combination\n",
    "experiment_colors = {}\n",
    "color_idx = 0\n",
    "\n",
    "for batch in config[\"batches\"]:\n",
    "    for experiment in config[\"experiments\"]:\n",
    "\n",
    "        # Use a single color for all trials of the same experiment\n",
    "        exp_key = f\"{batch}-{experiment}\"\n",
    "        \n",
    "        for trial in config[\"trials\"]:\n",
    "            checkpoint_path = Path(f\"{config['base_path']}/{batch}/{experiment}/{trial}/logs/attention.dat\")\n",
    "\n",
    "            if checkpoint_path.is_file():\n",
    "                with open(checkpoint_path, \"rb\") as handle:\n",
    "                    data = pickle.load(handle)\n",
    "                \n",
    "                for n_agents, att_data in data.items():\n",
    "                \n",
    "                    edge_indices = att_data[\"edge_indices\"]\n",
    "                    attention_weights = att_data[\"attention_weights\"]\n",
    "                    attention_over_time = att_data[\"attention_over_time\"]\n",
    "\n",
    "                    # Save plots\n",
    "                    match (experiment):\n",
    "                        case \"gat\" | \"graph_transformer\":\n",
    "\n",
    "                            # plot_attention_heatmap(experiment,edge_indices[-1], attention_weights[-1])\n",
    "\n",
    "                            # plot_attention_time_series(edge_indices, attention_weights, top_k=10)\n",
    "\n",
    "                            # for src_idx in [\n",
    "                            #     1,\n",
    "                            #     n_agents // 2,\n",
    "                            #     n_agents - 2,\n",
    "                            # ]:  # Plot for first token and middle token\n",
    "                            #     plot_node_attention_trends(experiment,edge_indices, attention_weights, source_node_idx=src_idx)\n",
    "\n",
    "                            pass\n",
    "\n",
    "\n",
    "                        case (\n",
    "                            \"transformer_full\"\n",
    "                        ):\n",
    "                            # if n_agents == 8:\n",
    "                            #     plot_transformer_attention(\n",
    "                            #         experiment,\n",
    "                            #         attention_weights[\"Enc_L0\"],\n",
    "                            #         \"Attention Weights \",\n",
    "                            #     )\n",
    "                            # Create grid visualizations\n",
    "                            # for attn_type in [\"Enc_L0\", \"Dec_L0\", \"Cross_L0\"]:\n",
    "                            #     if attn_type in attention_weights:\n",
    "                            #         for head_idx in range(2):  # Assuming 2 attention heads\n",
    "                            #             plot_attention_over_time_grid(\n",
    "                            #                 attention_over_time,\n",
    "                            #                 attn_type=attn_type,\n",
    "                            #                 head_idx=head_idx,\n",
    "                            #                 num_samples=5,\n",
    "                            #             )\n",
    "\n",
    "                            # Track key attention points\n",
    "                            # print(\"Creating trend plots...\")\n",
    "                            # for attn_type in [\"Enc_L0\", \"Dec_L0\", \"Cross_L0\"]:\n",
    "                            #     if attn_type in attention_weights:\n",
    "                            #         for head_idx in range(2):\n",
    "                            #             plot_key_attention_trends(\n",
    "                            #                 attention_over_time,\n",
    "                            #                 attn_type=attn_type,\n",
    "                            #                 head_idx=head_idx,\n",
    "                            #                 top_k=10,  # You can adjust this number\n",
    "                            #             )\n",
    "\n",
    "                            # Track attention from specific tokens\n",
    "                            if n_agents == 8 or n_agents == 24:\n",
    "                                for attn_type in [\"Dec_L0\"]:\n",
    "                                    if (attn_type in attention_over_time) and (attention_over_time[attn_type] != []):\n",
    "                                        # Determine how many attention heads we actually have\n",
    "                                        first_timestep = attention_over_time[attn_type][0]\n",
    "                                        num_heads = first_timestep.shape[1]\n",
    "                                        print(f\"Found {num_heads} attention heads for {attn_type}\")\n",
    "\n",
    "                                        \n",
    "                                        # Only iterate through available heads\n",
    "                                        # for head_idx in range(num_heads):\n",
    "                                        #     for src_idx in [\n",
    "                                        #         1,\n",
    "                                        #         n_agents // 2,\n",
    "                                        #         n_agents - 2,\n",
    "                                        #     ]:\n",
    "                                        #         plot_token_attention_trends(\n",
    "                                        #             experiment,\n",
    "                                        #             attention_over_time,\n",
    "                                        #             attn_type=attn_type,\n",
    "                                        #             src_idx=src_idx,\n",
    "                                        #             head_idx=head_idx,\n",
    "                                        #         )\n",
    "                                        \n",
    "                                        for src_idx in [\n",
    "                                            1,\n",
    "                                            n_agents // 2,\n",
    "                                            n_agents - 2,\n",
    "                                        ]:\n",
    "                                            plot_token_attention_trends(\n",
    "                                                experiment,\n",
    "                                                n_agents,\n",
    "                                                attention_over_time,\n",
    "                                                attn_type=attn_type,\n",
    "                                                src_idx=src_idx,\n",
    "                                                head_idx=1,\n",
    "                                            )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
